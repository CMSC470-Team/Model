# Project Writeup
## By Zihan Ma, Jake Baldwin, Abhay Patel, Seyed Ghaemi
---

## What we did?

### Basic model

First we used the sentenceBERT model ('all-mpnet-base-v2') to generate vector embeddings for entire sentences. 

Originally, we finetuned the BERT model by picking question pairs at random, then assigning scores to how well the sentences match using some basic features. 

For the basic features:
* Every question pair started with a score of 0.1
* We added 0.2 if they had the same category
* We added 0.6 if they had the same label

### Developing better features
At the same time, our plan was to change the features we 

---
## Why was it a good idea?

---
## Who did what?

---
## Did our technique work or not?